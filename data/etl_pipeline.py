"""ETL pipeline for loading engine production data into PostgreSQL.

Steps
-----
1. Extract: Read raw_production_data.csv generated by data_generator.py.
2. Transform: Clean data, standardize timestamps, and derive dimension tables.
3. Load: Upsert dimensions and insert fact rows into the star schema defined
   in schema.sql (FactProduction, DimMachine, DimOperator, DimDate).

This script is designed as a simple, modular, production-style example.
"""

from __future__ import annotations

import logging
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import pandas as pd
from sqlalchemy import Column, Integer, MetaData, Table, create_engine
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.engine import Engine
from sqlalchemy.exc import SQLAlchemyError


LOGGER = logging.getLogger(__name__)


@dataclass
class DbConfig:
	"""Database connection configuration.

	Values can be provided via environment variables or defaults.
	"""

	host: str = os.getenv("PGHOST", "localhost")
	port: int = int(os.getenv("PGPORT", "5432"))
	user: str = os.getenv("PGUSER", "postgres")
	password: str = os.getenv("PGPASSWORD", "postgres")
	database: str = os.getenv("PGDATABASE", "manufacturing_dw")

	def sqlalchemy_url(self) -> str:
		return (
			f"postgresql+psycopg2://{self.user}:{self.password}"
			f"@{self.host}:{self.port}/{self.database}"
		)


def configure_logging() -> None:
	logging.basicConfig(
		level=logging.INFO,
		format="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
	)


def get_engine(config: DbConfig) -> Engine:
	"""Create a SQLAlchemy engine for PostgreSQL."""

	LOGGER.info("Creating SQLAlchemy engine for PostgreSQL")
	engine = create_engine(config.sqlalchemy_url(), future=True)
	return engine


def extract(csv_path: Path) -> pd.DataFrame:
	"""Extract raw production data from CSV."""

	LOGGER.info("Extracting data from %s", csv_path)
	if not csv_path.exists():
		raise FileNotFoundError(f"Raw data file not found: {csv_path}")

	df = pd.read_csv(csv_path)
	LOGGER.info("Extracted %d rows with %d columns", df.shape[0], df.shape[1])
	return df


def _clean_and_standardize(df: pd.DataFrame) -> pd.DataFrame:
	"""Clean raw data and standardize timestamps.

	- Remove impossible torque values (< 0 or > 1000).
	- Handle missing values (drop critical rows, impute non-critical).
	- Parse timestamps to pandas Timestamps.
	"""

	LOGGER.info("Starting data cleaning and timestamp standardization")

	# Ensure expected columns exist
	expected_columns = {
		"engine_id",
		"assembly_line",
		"operator_id",
		"torque_value",
		"temperature_celsius",
		"cycle_time_seconds",
		"defect_flag",
		"downtime_minutes",
		"timestamp",
	}
	missing = expected_columns - set(df.columns)
	if missing:
		raise ValueError(f"Missing expected columns: {sorted(missing)}")

	# Remove impossible torque values
	before_rows = len(df)
	df = df[(df["torque_value"] >= 0) & (df["torque_value"] <= 1000)]
	LOGGER.info("Removed %d rows with impossible torque values", before_rows - len(df))

	# Standardize timestamps
	df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce", utc=True)

	# Handle missing values
	# Drop rows missing critical fields
	critical_cols = [
		"engine_id",
		"assembly_line",
		"operator_id",
		"torque_value",
		"temperature_celsius",
		"cycle_time_seconds",
		"defect_flag",
		"timestamp",
	]

	before_drop = len(df)
	df = df.dropna(subset=critical_cols)
	LOGGER.info("Dropped %d rows with missing critical fields", before_drop - len(df))

	# For non-critical numeric fields, fill missing with 0 or median
	if df["downtime_minutes"].isna().any():
		df["downtime_minutes"] = df["downtime_minutes"].fillna(0.0)

	# Ensure data types are consistent
	df["assembly_line"] = df["assembly_line"].astype(int)
	df["operator_id"] = df["operator_id"].astype(int)
	df["defect_flag"] = df["defect_flag"].astype(bool)

	return df


def _create_dim_date(df: pd.DataFrame) -> pd.DataFrame:
	"""Create DimDate from cleaned data timestamps."""

	LOGGER.info("Creating DimDate dimension")
	dates = df["timestamp"].dt.date
	dim_date = pd.DataFrame({"full_date": pd.to_datetime(dates)})
	dim_date = dim_date.drop_duplicates().reset_index(drop=True)

	dim_date["date_id"] = dim_date["full_date"].dt.strftime("%Y%m%d").astype(int)
	dim_date["day"] = dim_date["full_date"].dt.day.astype(np.int16)
	dim_date["month"] = dim_date["full_date"].dt.month.astype(np.int16)
	dim_date["year"] = dim_date["full_date"].dt.year.astype(np.int16)
	dim_date["week_number"] = dim_date["full_date"].dt.isocalendar().week.astype(np.int16)

	dim_date = dim_date[["date_id", "full_date", "day", "month", "year", "week_number"]]
	return dim_date


def _create_dim_machine(df: pd.DataFrame) -> pd.DataFrame:
	"""Create DimMachine from assembly_line.

	For this example, we model each assembly line as a machine.
	"""

	LOGGER.info("Creating DimMachine dimension")
	lines = (
		df[["assembly_line"]]
		.drop_duplicates()
		.sort_values("assembly_line")
		.reset_index(drop=True)
	)
	lines["machine_id"] = lines.index + 1
	lines["machine_description"] = (
		"Assembly Line " + lines["assembly_line"].astype(str)
	)

	dim_machine = lines[["machine_id", "assembly_line", "machine_description"]]
	return dim_machine


def _create_dim_operator(df: pd.DataFrame) -> pd.DataFrame:
	"""Create DimOperator from operator_id.

	Shift and experience_level are derived heuristically for demo purposes.
	"""

	LOGGER.info("Creating DimOperator dimension")
	ops = df[["operator_id"]].drop_duplicates().sort_values("operator_id").reset_index(drop=True)

	# Derive shift from operator_id buckets (purely synthetic logic)
	def derive_shift(op_id: int) -> str:
		if op_id % 3 == 0:
			return "Night"
		if op_id % 2 == 0:
			return "Evening"
		return "Day"

	def derive_experience(op_id: int) -> str:
		if op_id >= 140:
			return "Senior"
		if op_id >= 120:
			return "Mid"
		return "Junior"

	ops["shift"] = ops["operator_id"].astype(int).apply(derive_shift)
	ops["experience_level"] = ops["operator_id"].astype(int).apply(derive_experience)

	dim_operator = ops[["operator_id", "shift", "experience_level"]]
	return dim_operator


def _build_fact_production(
	df: pd.DataFrame,
	dim_date: pd.DataFrame,
	dim_machine: pd.DataFrame,
) -> pd.DataFrame:
	"""Create FactProduction-compatible DataFrame from cleaned data and dims."""

	LOGGER.info("Building FactProduction fact table frame")

	# Derive date_id directly from timestamps to avoid timezone/type mismatches
	df["date_id"] = df["timestamp"].dt.strftime("%Y%m%d").astype(int)

	# Map assembly_line to machine_id
	machine_map = dim_machine.set_index("assembly_line")["machine_id"].to_dict()
	df["machine_id"] = df["assembly_line"].map(machine_map)

	fact = pd.DataFrame(
		{
			"engine_id": df["engine_id"],
			"machine_id": df["machine_id"],
			"operator_id": df["operator_id"],
			"date_id": df["date_id"],
			"torque_value": df["torque_value"],
			"temperature_celsius": df["temperature_celsius"],
			"cycle_time_seconds": df["cycle_time_seconds"],
			"defect_flag": df["defect_flag"],
			"downtime_minutes": df["downtime_minutes"],
		}
	)

	# Basic data validation checks
	if fact.isna().any().any():
		missing_cols = fact.columns[fact.isna().any()].tolist()
		raise ValueError(f"FactProduction contains nulls in columns: {missing_cols}")

	if not ((fact["torque_value"] >= 0) & (fact["torque_value"] <= 1000)).all():
		raise ValueError("FactProduction contains torque_value outside [0, 1000]")

	return fact


def transform(df_raw: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
	"""Transform raw DataFrame into cleaned fact and dimension tables.

	Returns
	-------
	fact_production, dim_date, dim_machine, dim_operator
	"""

	LOGGER.info("Beginning transform step")
	df_clean = _clean_and_standardize(df_raw)

	dim_date = _create_dim_date(df_clean)
	dim_machine = _create_dim_machine(df_clean)
	dim_operator = _create_dim_operator(df_clean)
	fact_production = _build_fact_production(df_clean, dim_date, dim_machine)

	LOGGER.info(
		"Transform complete: fact=%d, dim_date=%d, dim_machine=%d, dim_operator=%d",
		len(fact_production),
		len(dim_date),
		len(dim_machine),
		len(dim_operator),
	)

	return fact_production, dim_date, dim_machine, dim_operator


def _reflect_tables(engine: Engine) -> Dict[str, Table]:
	"""Reflect existing tables from PostgreSQL using the schema in schema.sql."""

	LOGGER.info("Reflecting existing schema from database")
	metadata = MetaData()
	# Reflect all tables; PostgreSQL folds unquoted identifiers to lowercase,
	# so we normalize keys to lowercase for consistent lookup.
	metadata.reflect(bind=engine)
	tables = {tbl.name.lower(): tbl for tbl in metadata.sorted_tables}
	return tables


def _upsert_dimension(table: Table, df: pd.DataFrame, engine: Engine, key_columns: Tuple[str, ...]) -> None:
	"""Upsert rows into a dimension table using PostgreSQL ON CONFLICT."""

	LOGGER.info("Upserting %d rows into %s", len(df), table.name)
	if df.empty:
		return

	records = df.to_dict(orient="records")
	stmt = pg_insert(table).values(records)
	update_dict = {c.name: c for c in table.columns if c.name not in key_columns}
	if update_dict:
		stmt = stmt.on_conflict_do_update(index_elements=list(key_columns), set_=update_dict)
	else:
		stmt = stmt.on_conflict_do_nothing(index_elements=list(key_columns))

	with engine.begin() as conn:
		conn.execute(stmt)


def _insert_fact(table: Table, df: pd.DataFrame, engine: Engine) -> None:
	"""Insert fact rows (append-only)."""

	LOGGER.info("Inserting %d fact rows into %s", len(df), table.name)
	if df.empty:
		return

	records = df.to_dict(orient="records")
	with engine.begin() as conn:
		conn.execute(table.insert(), records)


def load(
	fact_production: pd.DataFrame,
	dim_date: pd.DataFrame,
	dim_machine: pd.DataFrame,
	dim_operator: pd.DataFrame,
	engine: Engine,
) -> None:
	"""Load dimension and fact tables into PostgreSQL."""

	LOGGER.info("Beginning load step")
	tables = _reflect_tables(engine)

	dim_date_tbl = tables.get("dimdate")
	dim_machine_tbl = tables.get("dimmachine")
	dim_operator_tbl = tables.get("dimoperator")
	fact_tbl = tables.get("factproduction")

	missing_any = any(
		tbl is None
		for tbl in [dim_date_tbl, dim_machine_tbl, dim_operator_tbl, fact_tbl]
	)
	if missing_any:
		missing = [
			name
			for name, tbl in [
				("DimDate", dim_date_tbl),
				("DimMachine", dim_machine_tbl),
				("DimOperator", dim_operator_tbl),
				("FactProduction", fact_tbl),
			]
			if tbl is None
		]
		raise RuntimeError(
			"Missing required tables in database. Ensure schema.sql has been applied. "
			f"Missing: {missing}"
		)

	# Upsert dimensions
	_upsert_dimension(dim_date_tbl, dim_date, engine, key_columns=("date_id",))
	_upsert_dimension(dim_machine_tbl, dim_machine, engine, key_columns=("machine_id",))
	_upsert_dimension(dim_operator_tbl, dim_operator, engine, key_columns=("operator_id",))

	# Insert facts
	_insert_fact(fact_tbl, fact_production, engine)

	LOGGER.info("Load step completed successfully")


def main() -> None:
	configure_logging()
	LOGGER.info("Starting ETL pipeline")

	csv_path = Path(__file__).with_name("raw_production_data.csv")
	config = DbConfig()

	try:
		df_raw = extract(csv_path)
		fact_production, dim_date, dim_machine, dim_operator = transform(df_raw)
		engine = get_engine(config)
		load(fact_production, dim_date, dim_machine, dim_operator, engine)
	except (FileNotFoundError, ValueError, SQLAlchemyError, RuntimeError) as exc:
		LOGGER.exception("ETL pipeline failed: %s", exc)
		raise
	else:
		LOGGER.info("ETL pipeline completed successfully")


if __name__ == "__main__":
	main()

